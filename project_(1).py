# -*- coding: utf-8 -*-
"""project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jD1HsqdY7-wC-37iLciY_Zg87_lxWs_3

# **Apple Stock Price Prediction**
"""

#packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#loading the data set
df=pd.read_csv("AAPL.csv",parse_dates=True)

df.head()

"""Dataset & Attributes:

The dataset consists of daily stock market data for Apple from 2012 to 2019, including:

üìå Stock Market Indicators:

‚óè	Date: Trading date

‚óè	Open: Opening price of Apple
stock for the day

‚óè	High: Highest price reached during the day

‚óè	Low: Lowest price reached during the day

‚óè	Close: Closing price of Apple stock for the day

‚óè	Volume: Number of shares traded on that day

üìå Target Variable:

‚óè	Next 30-Day Close Price Forecast

# Data Preprocessing:
"""

#handling missing values
df.isnull().sum()

# no null values

# checking duplicates
df.duplicated().sum()

# no duplicates

##Summary Statistics description
df.describe()

## data exploration
df.info()

#since date (object) needs to convert into date time"

##Converting Date to DateTime Object
df['Date']

df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)

df.dtypes

#since date(object ) is converted to date time

# convert Date as Index
df.set_index('Date',inplace=True)

df['Date']=df.index

df.head()

"""## Exploratory Data Analysis :"""

# moving averages('MA'):
df['MA07']=df['Close'].rolling(window=7).mean() #short term trend
df['MA30']=df['Close'].rolling(window=30).mean()#long term trend

# volatility measures
df['Volatility']=df['Close'].rolling(window=10).std()

# Daily returns for analyzing the stability
df['Daily_Returns']=df['Close'].pct_change()

df

#visualizations:

#stock price with moving averages
plt.figure(figsize=(15, 7))
plt.plot(df['Close'], label='Close Price')
plt.plot(df['MA07'], label='MA07 (7-Day Moving Average)')
plt.plot(df['MA30'], label='MA30 (30-Day Moving Average)')
plt.title('---Apple Stock Price with Moving Averages---')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend(fontsize='large')
plt.grid(True)
plt.show()

#Volatility & Volume
plt.plot()
plt.bar(df.index, df['Volume'], label='Volume', color='blue',alpha=0.3)
plt.ylabel('Volume', color='grey')
plt.legend(loc='upper left')

# Creating a twin axis to plot Volatility on the same chart
ax2 = plt.gca().twinx()
ax2.plot(df.index, df['Volatility'], label='Volatility (7-Day Std Dev)', color='red', linewidth=1)
ax2.set_ylabel('Volatility', color='red')
ax2.legend(loc='upper right')

plt.title('Trading Volume and Price Volatility')
plt.tight_layout()
plt.show()

# heatmap
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

# pairplot
sns.pairplot(df)
plt.tight_layout
plt.show()

# histogram
df.hist(bins=50, figsize=(15, 10))
plt.tight_layout()
plt.show()

##Checking Distribution of Our Data
fig = plt.figure(figsize=(10,8))
ax = sns.histplot(data=df, x='Adj Close', kde=True)
ax.set_title("Distribution Plot Adj Close -AAPL")
ax.tick_params(labelsize=12)
sns.set(font_scale=1)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.show()

#boxplot of apple stock price
fig = plt.figure(figsize=(10,6))
sns.boxplot(data=df, x='Adj Close', linewidth=2)
plt.title('boxplot of apple stock price', fontweight='bold', fontsize=14)
plt.xlabel('Adj Close', fontsize=12)
plt.ylabel('Values', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

#Year-wise Box Plot of Apple Stock Close Price
df['Year'] = df.index.year
plt.figure(figsize=(15, 7))
sns.boxplot(x='Year', y='Close', data=df, hue='Year', palette='viridis', legend=False)
plt.title('Year-wise Box Plot of Apple Stock Close Price',fontweight='bold', fontsize=14)
plt.xlabel('Year')
plt.ylabel('Close Price')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

#Processed Data
print("\nProcessed Data :")
print(df[['Close', 'MA07', 'MA30', 'Volatility', 'Daily_Returns']].tail())

#Time Series Analysis package to find seasonality
from statsmodels.tsa.seasonal import seasonal_decompose

#  Seasonal Decomposition
# We use a period of 252 for daily stock data to capture annual seasonality (approx 252 trading days/year)
# 'Multiplicative' model is often better for stocks where variations grow with the price.
result=seasonal_decompose(df['Close'],model='multiplicative',period=252)

#visualization
plt.figure(figsize=(15, 13))

#observed--actual price
plt.subplot(4,1,1)
plt.plot(result.observed, label='Observed', color='blue')
plt.legend(loc='upper left')
plt.title('Seasonal Decomposition of Apple Stock Price')

# Trend ---Moving Average / Long term direction
plt.subplot(4,1,2)
plt.plot(result.trend, label='Trend', color='green')
plt.legend(loc='upper left')

# Seasonality --- Repeating patterns
plt.subplot(4,1,3)
plt.plot(result.seasonal, label='Seasonality', color='orange')
plt.legend(loc='upper left')


# Residuals---Noise / Unexplained variance
plt.subplot(4,1,4)
plt.plot(result.resid, label='Residuals', color='red')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

"""Observations of the Output:

Observed: This represents the original, raw time series data of the Apple stock's closing price, exactly as it was input into the decomposition model.

Trend: Shows the overall direction of Apple's stock price over the years, filtering out short-term fluctuations. This aligns with the goal to "Analyze short-term and long-term trends".


Seasonality: Reveals repeating patterns. For example, if the stock consistently rises or falls at certain times of the year (like around product launches in September), this chart will capture it.


Residuals: This is the random noise left after removing the trend and seasonality. Large spikes here indicate unexpected events (like the "external events" mentioned in the goals ).

## model evaluation:
"""

from statsmodels.tsa.stattools import adfuller
print(f'Augmented Dickey-Fuller Test:')
result=adfuller(df['Close'])
print(f'ADF statistic: {result[0]}')
print(f'p-value: {result[1]}')

if result[1] <= 0.05:
  print('result is stationary')
else:
  print('result is non-stationary')

"""# acf and pacf"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# long term acf
plt.figure(figsize=(10, 6))
plot_acf(df['Close'], lags=40)
plt.title('Long-Term Autocorrelation Function (ACF)')
plt.tight_layout()
plt.show()

#short term acf

plt.figure(figsize=(10, 6))
plot_acf(df['Close'], lags=10)
plt.title('Short-Term Autocorrelation Function (ACF) ')
plt.tight_layout()
plt.show()

# long term pacf
plt.figure(figsize=(10, 6))
plot_pacf(df['Close'], lags=40)
plt.title('Long-Term Partial Autocorrelation Function (PACF) ')
plt.tight_layout()
plt.show()

#short term pacf

plt.figure(figsize=(10, 6))
plot_pacf(df['Close'], lags=10)
plt.title('Short-Term Partial Autocorrelation Function (PACF)')
plt.tight_layout()
plt.show()

# Interpretation of ACF and PACF Plots for the original 'Close' price series
# The original 'Close' price series was found to be non-stationary (ADF p-value: 0.996).

# ACF Plot Observations:
# The Autocorrelation Function (ACF) values are high and decay very slowly, remaining significant for many lags (up to 40 and likely beyond).
# This slow decay is a classic indicator of a non-stationary time series, suggesting a strong trend or persistent dependencies over time.

# PACF Plot Observations:
# The Partial Autocorrelation Function (PACF) plot typically shows a very significant spike at the first lag, followed by a rapid drop-off, though some smaller spikes may appear at later lags.
# This pattern, combined with the slowly decaying ACF, further confirms the non-stationarity of the series.

# Implications for ARIMA Modeling:
# For effective ARIMA model parameter selection (p and q), ACF and PACF plots should be analyzed on a stationary time series.
# Since the original 'Close' price series is non-stationary, these plots do not provide clear 'cut-off' points to directly determine the 'p' (AR order) and 'q' (MA order) parameters.
# The 'Close_Differenced' series, which was confirmed to be stationary, would be the appropriate series for interpreting ACF and PACF to determine ARIMA parameters.

#converting into stationary
df['Close_Differenced'] = df['Close'].diff(1)
print(f'Augmented Dickey-Fuller Test on Differenced Series:')
result_diff = adfuller(df['Close_Differenced'].dropna())
print(f'ADF statistic: {result_diff[0]}')
print(f'p-value: {result_diff[1]}')

if result_diff[1] <= 0.05:
  print('result is stationary')
else:
  print('result is non-stationary')

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

# Plot ACF for the differenced series
plt.figure(figsize=(10, 6))
plot_acf(df['Close_Differenced'].dropna(), lags=40)
plt.title('Autocorrelation Function (ACF) for Differenced Close Price')
plt.tight_layout()
plt.show()

# Plot PACF for the differenced series
plt.figure(figsize=(10, 6))
plot_pacf(df['Close_Differenced'].dropna(), lags=40)
plt.title('Partial Autocorrelation Function (PACF) for Differenced Close Price')
plt.tight_layout()
plt.show()

"""## model -1  -arima"""

from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings("ignore")

# 1. Define Split Ratio (80% Train, 20% Test)
train_size = int(len(df) * 0.8)

train_data = df['Close'].iloc[:train_size]
test_data = df['Close'].iloc[train_size:]

# 3. Verify the Split
print(f"Total Observations: {len(df)}")
print(f"Training Set: {len(train_data)} rows (Ends: {train_data.index[-1].date()})")
print(f"Testing Set:  {len(test_data)} rows (Starts: {test_data.index[0].date()})")

# 4. Visualize the Split
plt.figure(figsize=(12, 6))
plt.plot(train_data, label='Training Data (History)')
plt.plot(test_data, label='Testing Data (Future/Validation)', color='orange')
plt.title("Train vs Test Data Split")
plt.legend()
plt.show()

from statsmodels.tsa.arima.model import ARIMA

import itertools
import warnings
from statsmodels.tsa.arima.model import ARIMA

# Ignore warnings to keep output clean
warnings.filterwarnings("ignore")

# Define the range of parameters to test
# We test p (0-5), d (1), q (0-5)
p_values = range(0, 6)
d_values = [1] # We know d=1 is likely correct
q_values = range(0, 6)

# Generate all different combinations of p, d, q
pdq_combinations = list(itertools.product(p_values, d_values, q_values))

best_aic = float("inf")
best_order = None

print("Running Grid Search to find optimal (p,d,q)...")

for order in pdq_combinations:
    try:
        # Train the model with the current combination
        model = ARIMA(train_data, order=order)
        model_fit = model.fit()

        # Lower AIC means a better model
        if model_fit.aic < best_aic:
            best_aic = model_fit.aic
            best_order = order
            print(f"New Best: {order} | AIC: {best_aic}")

    except:
        continue

print("\n----------------------------------------")
print(f"OPTIMAL PARAMETERS FOUND: {best_order}")
print(f"Lowest AIC Score: {best_aic}")
print("----------------------------------------")

print("best order is :",best_order)

final_model=ARIMA(train_data,order=best_order)
final_fit=final_model.fit()
print(final_fit.summary())

from sklearn.metrics import mean_squared_error,root_mean_squared_error,mean_absolute_percentage_error

# 1. Train the Model ONCE on the Training Data
# Reverting to the optimal order found by grid search
model=ARIMA(train_data,order=best_order)
model_fit=model.fit()

# 2. Predict the entire Test Set at once
# We ask the model to forecast 'n' steps into the future, where 'n' is the length of our test data
forecast = model_fit.forecast(steps=len(test_data))

# 3. Quick Error Check (RMSE)
# This tells you, on average, how far off the price prediction is (in dollars)
mse = mean_squared_error(test_data, forecast)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

mape=mean_absolute_percentage_error(test_data,forecast)
print(f"mean absolute percentage error (mape): {mape:.2f}")

#plot for result
plt.figure(figsize=(10,5))
plt.plot(test_data.index, test_data, label='Actual Price', color='blue')
plt.plot(test_data.index, forecast, label='Predicted Price', color='red', linestyle='--')
plt.title('Validation: Actual vs Predicted Prices')
plt.legend()
plt.show()

import joblib

# Save the final_model using joblib
joblib.dump(final_model, 'arima_model.joblib')
print("Model saved as 'arima_model.joblib'")

!pip install streamlit
!pip install joblib
!pip install matplotlib
import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt

st.set_page_config(layout="wide")
st.title("Apple Stock Price Prediction using ARIMA")

# Load the pre-trained ARIMA model
# Make sure 'arima_model.joblib' is in the same directory as this Streamlit app
try:
    model = joblib.load('arima_model.joblib')
    st.success("ARIMA model loaded successfully!")
except FileNotFoundError:
    st.error("Error: 'arima_model.joblib' not found. Please ensure the model file is in the correct directory.")
    st.stop()

# Get the last date from the training data for forecasting context
# Assuming train_data was used to fit the model and is available (or can be inferred)
# For simplicity, let's use the last date from the original df if it's still in memory
# In a real deployment, you'd save/load this info with the model or derive from new data

# To make this self-contained for deployment, let's assume we can load enough historical data
# or pass the last observed value to the model for forecasting.
# For now, we'll use the 'train_data' global variable from the notebook context for last date.
# In a deployed app, you'd pass the last 'n' observations to the forecast method.

# We need the last observation from the training data to start forecasting
# As 'train_data' is a Series, its last index is the last date it contains
last_train_date = train_data.index[-1]
last_train_value = train_data.iloc[-1]

st.write(f"Model trained up to: {last_train_date.strftime('%Y-%m-%d')}")
st.write(f"Last observed closing price (from training data): ${last_train_value:.2f}")

st.sidebar.header("Prediction Settings")
num_days = st.sidebar.slider("Number of days to forecast:", min_value=1, max_value=90, value=30)

if st.sidebar.button("Generate Forecast"):
    st.subheader(f"Forecasting Apple Stock Price for the next {num_days} days")

    # Generate forecast
    # The forecast method of a fitted ARIMA model needs the `steps` parameter
    # and will continue from the end of the data it was fitted on.
    try:
        forecast = model.predict(start=len(train_data), end=len(train_data) + num_days - 1)

        # Create a date range for the forecast
        forecast_dates = pd.date_range(start=last_train_date + pd.Timedelta(days=1), periods=num_days, freq='B') # 'B' for business day

        forecast_df = pd.DataFrame({'Date': forecast_dates, 'Predicted Close': forecast.values})
        forecast_df.set_index('Date', inplace=True)

        st.write("### Predicted Prices:")
        st.dataframe(forecast_df)

        # Plotting the forecast
        st.write("### Forecast Visualization:")
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(train_data.index, train_data, label='Historical Training Data', color='blue')
        # Check if test_data exists and plot it if available
        if 'test_data' in globals() and not test_data.empty:
            ax.plot(test_data.index, test_data, label='Actual Test Data', color='orange')
        ax.plot(forecast_df.index, forecast_df['Predicted Close'], label='Forecasted Prices', color='red', linestyle='--')
        ax.set_title('Apple Stock Price Forecast')
        ax.set_xlabel('Date')
        ax.set_ylabel('Close Price')
        ax.legend()
        ax.grid(True)
        st.pyplot(fig)

    except Exception as e:
        st.error(f"An error occurred during forecasting: {e}")


st.markdown("---")
st.write("This is a demonstration of an ARIMA model for stock price prediction.")
st.write("Disclaimer: Stock market predictions are inherently uncertain. This tool is for educational purposes only and should not be considered financial advice.")